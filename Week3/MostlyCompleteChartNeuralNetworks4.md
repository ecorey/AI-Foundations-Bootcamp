![NN Chart](./NeuralNetworks.png)
[source](https://www.asimovinstitute.org/neural-network-zoo/)

---

- Perceptron (P): The simplest form of a neural network, consisting of a single neuron. It is suitable for linearly separable problems.

- Feed Forward (FF): A simple neural network where connections between the units do not form a cycle. This is the first and simplest type of artificial neural network devised.

- Radial Basis Function Network (RBF): Uses radial basis functions as activation functions. It is primarily used for function approximation and interpolation.

- Recurrent Neural Network (RNN): Possesses connections between nodes form a directed graph along a temporal sequence, allowing it to exhibit temporal dynamic behavior.

- Long Short-Term Memory (LSTM): A special kind of RNN capable of learning long-term dependencies, using special units called memory cells.

- Gated Recurrent Unit (GRU): Similar to an LSTM, but uses a simplified gating mechanism. GRUs combine the forget and input gates into a single "update gate."

- Auto Encoder (AE): Used to learn efficient codings in an unsupervised manner. It is designed to reproduce its input at its output.

- Variational AE (VAE): A type of AE that creates a directed graphical model using variational inference and is used for generative purposes.

- Denoising AE (DAE): An AE that is trained to use a corrupted version of its input to recover the original undistorted input.

- Sparse AE (SAE): An AE that enforces a sparsity constraint on its hidden units during training.

- Markov Chain (MC): A stochastic model that describes a sequence of possible events, where the probability of each event depends only on the state attained in the previous event.

- Hopfield Network (HN): A form of recurrent artificial neural network that serves as content-addressable memory systems with binary threshold nodes.

- Boltzmann Machine (BM): A type of stochastic recurrent neural network and is one of the earliest types of neural networks capable of learning internal representations.

- Restricted BM (RBM): A variant of BM with a restricted architecture that allows for more efficient training algorithms.

- Deep Belief Network (DBN): A stack of RBMs where each RBM layer communicates with both the previous and subsequent layers.

- Deep Convolutional Network (DCN): An artificial neural network that uses convolutional layers to process data in grid-like topology. It's widely used in image recognition.

- Deconvolutional Network (DN): Works in an opposite manner to convolutional networks. It is used in image segmentation tasks.

- Deep Convolutional Inverse Graphics Network (DCIGN): A network that infers a three-dimensional representation of images and is used for tasks that require understanding the 3D aspects of input.
